{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77db741a-d9b8-4cd4-acdd-c2477e5e6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import io\n",
    "import json\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode - o\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "lang_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "import vima_bench\n",
    "from vima_bench import PARTITION_TO_SPECS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0f9ffc-1863-416a-887e-44f008b71c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "                \n",
    "# helper mlp init function\n",
    "def mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod=None):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# MLP policy\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim, action_dim, hidden_dim, hidden_depth, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod)\n",
    "        self.outputs = dict()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "\n",
    "# MLP policy\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, action_dim, hidden_size, mask=False, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,32,kernel_size=8,stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,72)=>(b_size,32,8,17)\n",
    "            nn.Conv2d(32,64,kernel_size=4,stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,17)=>(b_size,64,3,7)\n",
    "            nn.Conv2d(64,32,kernel_size=3,stride=1), nn.LeakyReLU(inplace=True), Flatten(), nn.BatchNorm1d(32*1*5), #(b_size,64,3,7)=>(b_size,32,1,5)=>(b_size,32*1*5)\n",
    "            nn.Linear(32*1*5, hidden_size) #(b_size,32*1*5)=>(b_size,hidden_size)\n",
    "        )\n",
    "        self.process = mlp(hidden_size, 1*36*72, hidden_dim=1000, hidden_depth=1, output_mod=nn.Sigmoid()) #(b_size,hidden_size*2)=>(b_size,32*1*1)\n",
    "        self.cnntrunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,72)=>(b_size,32,8,17)\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,17)=>(b_size,64,3,7)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1), nn.LeakyReLU(inplace=True), nn.BatchNorm2d(32), Flatten(), #(b_size,64,3,7)=>(b_size,32,1,5)=>(b_size,32*1*5)\n",
    "            nn.Linear(32*1*5, action_dim)#, nn.LeakyReLU(inplace=True), nn.BatchNorm1d(32), #(b_size,32*1*5)=>(b_size,action_dim)\n",
    "        )\n",
    "        self.mlptrunk = mlp(hidden_size*2, action_dim, hidden_dim=100, hidden_depth=1) #(b_size,hidden_size*2)=>(b_size,action_dim)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state/255.0 # process image + switch channels\n",
    "        state = state.permute(0,3,1,2)\n",
    "        \n",
    "        if self.mask:\n",
    "            state_embed = self.conv(state)\n",
    "            img_mask = self.process(state_embed)\n",
    "            img_mask = img_mask.reshape(-1,1,36,72)\n",
    "            masked_state = state * img_mask # apply mask to full state\n",
    "            pred = self.cnntrunk(masked_state)\n",
    "            return [pred, masked_state, img_mask]\n",
    "        else:\n",
    "            pred = self.cnntrunk(state)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e07cbd67-71e4-4c60-8662-070077913cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsize_obs(obs):\n",
    "    cv2.imwrite('temp.jpg', obs)\n",
    "    img = cv2.imread('temp.jpg')\n",
    "    downsized_obs = cv2.resize(img, dsize=(72, 36), interpolation=cv2.INTER_CUBIC)\n",
    "    return downsized_obs\n",
    "\n",
    "def flatten_act(action):\n",
    "    return np.concatenate(list(action.values())).ravel()\n",
    "\n",
    "# reconstructs actions for simulator\n",
    "def reconstruct_act(action):\n",
    "    reconst_action = {}\n",
    "    reconst_action['pose0_position'] = np.array(action[0:2])\n",
    "    reconst_action['pose0_rotation'] = np.array(action[2:6])\n",
    "    reconst_action['pose1_position'] = np.array(action[6:8])\n",
    "    reconst_action['pose1_rotation'] = np.array(action[8:12])\n",
    "    reconst_action = {\n",
    "        k: np.clip(v, env.action_space[k].low, env.action_space[k].high)\n",
    "        for k, v in reconst_action.items()\n",
    "    }\n",
    "    return reconst_action\n",
    "\n",
    "def remove_obj(segm, obs, remove_obj):\n",
    "    #if remove_obj == 'arm':\n",
    "    #    segm = (segm == 2).astype(int)\n",
    "    if remove_obj == 'base':\n",
    "        segm = (segm == 5).astype(int)\n",
    "    elif remove_obj == 'dragged':\n",
    "        segm = (segm == 6).astype(int)\n",
    "    elif remove_obj == 'distractor':\n",
    "        segm = (segm == 7).astype(int)\n",
    "    segm = np.atleast_3d(segm)\n",
    "    \n",
    "    for height in range(128):\n",
    "        for width in range(256):\n",
    "            if segm[height, width] == 1:\n",
    "                obs[height, width] = 47\n",
    "    return obs\n",
    "\n",
    "def compare_actions(actions, gt_action):\n",
    "    comparisons = []\n",
    "    for action in actions:\n",
    "        comparisons.append(np.linalg.norm(gt_action - action[[0,1,6,7]],ord=2))\n",
    "    return comparisons\n",
    "\n",
    "# generates random trajs within specified constraints\n",
    "def gen_trajs(env, num_trajs, task_name, task_kwargs):\n",
    "    trajs = []\n",
    "    task = env.task\n",
    "    oracle_fn = task.oracle(env)\n",
    "    for traj in tqdm(range(num_trajs)):\n",
    "        traj = {'obs': [],'acts': [], 'meta': []}\n",
    "        obs = env.reset()\n",
    "        traj['meta'] = env.meta_info\n",
    "        for step in range(1):\n",
    "            top_obs = obs['rgb']['top'] # extracts top down view only\n",
    "            top_obs = np.rollaxis(top_obs,0,3)\n",
    "            traj['obs'].append(downsize_obs(top_obs.copy()))\n",
    "            # prompt, prompt_assets = env.prompt, env.prompt_assets\n",
    "            oracle_action = oracle_fn.act(obs)\n",
    "            # clip action\n",
    "            oracle_action = {\n",
    "                k: np.clip(v, env.action_space[k].low, env.action_space[k].high)\n",
    "                for k, v in oracle_action.items()\n",
    "            }\n",
    "            traj['acts'].append(flatten_act(oracle_action))\n",
    "            obs, _, done, info = env.step(action=oracle_action, skip_oracle=False)\n",
    "        traj['obs'] = np.array(traj['obs'])\n",
    "        traj['acts'] = np.array(traj['acts'])\n",
    "        traj['meta'] = np.array(traj['meta'])\n",
    "        trajs.append(traj)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfc2f6-2659-4640-b9d6-0a7546455ef9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_name = 'visual_manipulation'\n",
    "#task_kwargs=PARTITION_TO_SPECS[\"train\"][task_name]\n",
    "task_kwargs = { 'num_dragged_obj': 1,\n",
    "     'possible_dragged_obj': ['flower'],\n",
    "     'possible_dragged_obj_texture': ['purple'],\n",
    "     'dragged_obj_area': [1],\n",
    "     'num_base_obj': 1,\n",
    "     'possible_base_obj': ['bowl'],\n",
    "     'possible_base_obj_texture': ['green'],\n",
    "     'base_obj_area': [4],\n",
    "     'num_distractors_obj': 0,\n",
    "     'possible_distractor_obj_texture': ['yellow'],\n",
    "     'possible_distractor_obj': ['flower'],\n",
    "     'distractor_obj_area' : 3}\n",
    "#record_gui=True, display_debug_window=True, hide_arm_rgb=False\n",
    "env = vima_bench.make(task_name=task_name,task_kwargs=task_kwargs,hide_arm_rgb=False)\n",
    "\n",
    "num_trajs = 10\n",
    "trajs = gen_trajs(env=env, num_trajs=num_trajs, task_name=task_name, task_kwargs=task_kwargs)\n",
    "pickle.dump(trajs, open('trajs.pkl', 'wb'))\n",
    "env.close()\n",
    "#obs = env.reset()\n",
    "#top_obs = obs['rgb']['top']\n",
    "#top_obs = np.rollaxis(top_obs,0,3)\n",
    "#plt.imshow(top_obs)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95620041-1b7d-47f7-a043-eed9b05e824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trajs = pickle.load(open('trajs.pkl','rb'))\n",
    "plt.imshow(trajs[0]['obs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b88c2-3372-4b12-b64e-3b7d4cf43f4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tasks = len(trajs)\n",
    "\n",
    "#goal_size = 8 #384\n",
    "act_size = trajs[0]['acts'][0].shape[0]\n",
    "hidden_size = 100\n",
    "mask = False\n",
    "\n",
    "policy = CNNPolicy(act_size, hidden_size, mask=mask)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde3ee5-ce6a-41eb-ac85-860781776e8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 10\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t_idx = np.random.randint(len(trajs), size=(batch_size,)) # Indices of traj\n",
    "        t_idx_pertraj = np.random.randint(1, size=(batch_size,)) # Indices of timesteps in traj\n",
    "        t_states = np.concatenate([trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_actions = np.concatenate([trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "   \n",
    "        t_states = torch.Tensor(t_states).float().to(device)\n",
    "        t_actions = torch.Tensor(t_actions).float().to(device)\n",
    "        \n",
    "        if mask:\n",
    "            a_preds, masked_state_preds, img_mask_preds = policy(t_states)\n",
    "            mask_loss = .00001*masked_state_preds.sum()\n",
    "        else:\n",
    "            a_preds = policy(t_states)[0]\n",
    "        \n",
    "        action_loss = torch.mean(torch.linalg.norm(a_preds - t_actions, dim=-1)) # supervised learning loss\n",
    "        if mask:\n",
    "            loss = action_loss + mask_loss\n",
    "        else:\n",
    "            loss = action_loss\n",
    "            mask_loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(policy.conv[0].weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] mask loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, mask_loss))\n",
    "            print('[%d, %5d] action loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, action_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "torch.save(policy, 'policy.pt')\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee1bd52a-f3e5-41d7-b327-cb9bf03e20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"visual_manipulation\"\n",
    "task_kwargs = { 'num_dragged_obj': 1,\n",
    "     'possible_dragged_obj': ['block'],\n",
    "     'possible_dragged_obj_texture': ['red'],\n",
    "     'dragged_obj_area': [1],\n",
    "     'num_base_obj': 1,\n",
    "     'possible_base_obj': ['pan'],#['bowl','square','flower','pentagon','letter E'],\n",
    "     'possible_base_obj_texture': ['tiger'],#['yellow','blue','green','tiger','dark green swirl'],\n",
    "     'base_obj_area': [4],\n",
    "     'num_distractors_obj': 0,\n",
    "     'possible_distractor_obj_texture': ['tiger'],\n",
    "     'possible_distractor_obj': ['pan'],\n",
    "     'distractor_obj_area' : [3]}\n",
    "record_cfg = {'save_video': True,\n",
    "     'save_video_path': './rollouts/',\n",
    "     'view': 'front',\n",
    "     'fps': 20,\n",
    "     'video_height': 640,\n",
    "     'video_width': 720}\n",
    "# record_gui=True, display_debug_window=True, hide_arm_rgb=False\n",
    "env = vima_bench.make(task_name=task_name,task_kwargs=task_kwargs,hide_arm_rgb=False,record_cfg=record_cfg)\n",
    "#obs = env.reset()\n",
    "#segm = obs['segm']['top']\n",
    "#top_obs = obs['rgb']['top']\n",
    "#top_obs = np.rollaxis(top_obs,0,3)\n",
    "#plt.imshow(remove_obj(segm, top_obs, 'dragged'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10160fc2-8e9d-4d65-8517-3421c5fae5d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: movej exceeded 5 second timeout. Skipping.\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = torch.load('policy.pt')\n",
    "\n",
    "obj_to_remove = None#['base']\n",
    "mask = False\n",
    "policy.eval()\n",
    "num_test_trajs = 1\n",
    "video = True\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(0.3, 0.7)\n",
    "\n",
    "successes = []\n",
    "\n",
    "rollouts = {'actions': [], 'action_starts': [],'action_ends': [], 'true_starts': [], 'true_ends': []}\n",
    "for i in tqdm(range(num_test_trajs)):\n",
    "    os.makedirs('rollouts/' + str(i), exist_ok=True)\n",
    "    obs = env.reset()\n",
    "    rollouts['true_starts'].append(env.task.dragged_pose[0])\n",
    "    rollouts['true_ends'].append(env.task.base_pose[0])\n",
    "    if video:\n",
    "        video_name = str(i)\n",
    "        env.start_rec(video_name)\n",
    "    for step in range(1):\n",
    "        segm = obs['segm']['top']\n",
    "        top_obs = obs['rgb']['top']\n",
    "        top_obs = np.rollaxis(top_obs,0,3)\n",
    "        if obj_to_remove is not None:\n",
    "            obj_removed = random.choice(obj_to_remove)\n",
    "            segm = obs['segm']['top']\n",
    "            top_obs = remove_obj(segm, top_obs, obj_removed)\n",
    "        first_obs = top_obs.copy()\n",
    "        im = Image.fromarray(top_obs)\n",
    "        im.save('rollouts/'+str(i)+\"/\"+str(step)+'.jpg')\n",
    "        top_obs = downsize_obs(top_obs)\n",
    "        state = torch.Tensor(top_obs[None]).to(device)\n",
    "        if mask:\n",
    "            action, masked_state, img_mask = policy(state)\n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            masked_state = masked_state.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            masked_state = Image.fromarray((masked_state * 255).astype(np.uint8))\n",
    "            masked_state.save('rollouts/'+str(i)+\"/\"+str(step)+'-mask.jpg')\n",
    "            img_mask = img_mask.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "        else:\n",
    "            action = policy(state).cpu().detach().numpy()[0]\n",
    "        rollouts['actions'].append(action)\n",
    "        rollouts['action_starts'].append(action[0:2].copy())\n",
    "        rollouts['action_ends'].append(action[6:8].copy())\n",
    "        obs, _, done, info = env.step(action=reconstruct_act(action), skip_oracle=False)\n",
    "        if done:\n",
    "            successes.append(1)\n",
    "        else:\n",
    "            successes.append(0)\n",
    "    if video:\n",
    "        env.end_rec()\n",
    "    top_obs = obs['rgb']['top']\n",
    "    top_obs = np.rollaxis(top_obs,0,3)\n",
    "    if obj_to_remove is not None:\n",
    "        segm = obs['segm']['top']\n",
    "        top_obs = remove_obj(segm, top_obs, obj_removed)\n",
    "    im = Image.fromarray(top_obs)\n",
    "    im.save('rollouts/'+str(i)+\"/\"+str(step+1)+'.jpg')\n",
    "\n",
    "env.close()\n",
    "plt.scatter(np.array(rollouts['true_starts'])[:, 1], np.array(rollouts['true_starts'])[:, 0], marker='o', color='g', label='gt starts')\n",
    "plt.scatter(np.array(rollouts['true_ends'])[:, 1], np.array(rollouts['true_ends'])[:, 0], marker='x', color='g', label='gt ends')\n",
    "plt.scatter(np.array(rollouts['action_starts'])[:, 1], np.array(rollouts['action_starts'])[:, 0], marker='o', color='r', label='rollout starts')\n",
    "plt.scatter(np.array(rollouts['action_ends'])[:, 1], np.array(rollouts['action_ends'])[:, 0], marker='x', color='r', label='rollout ends')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "print(sum(successes)/len(successes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae59c2-c87f-4e2f-a91b-e99144f02302",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d61716-d37b-4296-b93d-39bd26c97ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336d9f0-b371-4a58-ac38-f782648bdab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_action = np.array([ 0.348276  , -0.28591025,  0.5994489 ,  0.13083145])\n",
    "comps = compare_actions(rollouts['actions'], gt_action)\n",
    "print(comps)\n",
    "print(sum(comps)/len(comps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a4693-f77c-43bc-ae56-351eed20ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(obs):\n",
    "    policy.eval()\n",
    "    policy.to(device)\n",
    "    \n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = policy(obs).detach().cpu().numpy()\n",
    "    \n",
    "    dists = np.linalg.norm(logits - gt_action.reshape(1, 12), axis=1).reshape(-1, 1)\n",
    "    # d2 = -dists.copy()\n",
    "    # val = np.concatenate((dists, d2), axis=1)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ee30a-693f-4f5e-b401-f29be7516d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(downsize_obs(first_obs), \n",
    "                                         batch_predict, # prediction function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function\n",
    "print(explanation)\n",
    "print(explanation.intercept)\n",
    "print(explanation.local_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd11541-f5c9-41c2-86d6-805fd168236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation)\n",
    "print(explanation.intercept)\n",
    "print(explanation.local_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5305e-719b-422d-b71b-9f2e9560d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(explanation.segments)\n",
    "print(explanation.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31fd8a-b816-40ff-a932-6c656b7118bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_cam(policy, state):\n",
    "    policy.eval()\n",
    "    policy.to(device)\n",
    "        \n",
    "    state = state/255.0 # process image + switch channels\n",
    "    state = state.permute(0,3,1,2)\n",
    "    conv_maps = policy.cnntrunk[2](policy.cnntrunk[1](policy.cnntrunk[0](state)))\n",
    "    conv_maps = policy.cnntrunk[5](policy.cnntrunk[4](policy.cnntrunk[3](conv_maps)))\n",
    "    conv_maps = policy.cnntrunk[9](policy.cnntrunk[8](policy.cnntrunk[7](policy.cnntrunk[6](conv_maps)))).detach().numpy()[0]\n",
    "    weights = policy.cnntrunk[10].weight.data.cpu().numpy()[0]\n",
    "    \n",
    "    avg_conv_map = np.uint8(conv_maps * weights *255)\n",
    "    avg_conv_map[avg_conv_map < 0] = 0\n",
    "    avg_conv_map = np.array(Image.fromarray(avg_conv_map).resize((256,128)))\n",
    "    \n",
    "    return avg_conv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb35a5-495e-47ea-9de8-d572bcdd9c05",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = grad_cam(policy,state)\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df446a-ee2c-4a01-ad09-41bf30548540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap = cv2.applyColorMap(cv2.resize(output,(256, 128)), cv2.COLORMAP_JET)\n",
    "plt.imshow(heatmap*.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5618114-d080-41a1-a026-c2b2cefa5dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aligning-construals] *",
   "language": "python",
   "name": "conda-env-aligning-construals-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
