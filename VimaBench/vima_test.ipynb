{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77db741a-d9b8-4cd4-acdd-c2477e5e6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jun  2 2023 13:47:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 17 tasks loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import io\n",
    "import json\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode - o\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "#lang_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "import vima_bench\n",
    "from vima_bench import PARTITION_TO_SPECS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0f9ffc-1863-416a-887e-44f008b71c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "                \n",
    "# helper mlp init function\n",
    "def mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod=None):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# MLP policy\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim, action_dim, hidden_dim, hidden_depth, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod)\n",
    "        self.outputs = dict()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "\n",
    "# MLP policy\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, action_dim, hidden_size, mask=False, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,32,kernel_size=8,stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,72)=>(b_size,32,8,17)\n",
    "            nn.Conv2d(32,64,kernel_size=4,stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,17)=>(b_size,64,3,7)\n",
    "            nn.Conv2d(64,32,kernel_size=3,stride=1), nn.LeakyReLU(inplace=True), Flatten(), nn.BatchNorm1d(32*1*5), #(b_size,64,3,7)=>(b_size,32,1,5)=>(b_size,32*1*5)\n",
    "            nn.Linear(32*1*5, hidden_size) #(b_size,32*1*5)=>(b_size,hidden_size)\n",
    "        )\n",
    "        self.process = mlp(hidden_size, 1*36*72, hidden_dim=1000, hidden_depth=1, output_mod=nn.Sigmoid()) #(b_size,hidden_size*2)=>(b_size,32*1*1)\n",
    "        self.cnntrunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,72)=>(b_size,32,8,17)\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,17)=>(b_size,64,3,7)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1), nn.LeakyReLU(inplace=True), nn.BatchNorm2d(32), Flatten(), #(b_size,64,3,7)=>(b_size,32,1,5)=>(b_size,32*1*5)\n",
    "            nn.Linear(32*1*5, action_dim)#, nn.LeakyReLU(inplace=True), nn.BatchNorm1d(32), #(b_size,32*1*5)=>(b_size,action_dim)\n",
    "        )\n",
    "        self.mlptrunk = mlp(hidden_size*2, action_dim, hidden_dim=100, hidden_depth=1) #(b_size,hidden_size*2)=>(b_size,action_dim)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state/255.0 # process image + switch channels\n",
    "        state = state.permute(0,3,1,2)\n",
    "        \n",
    "        if self.mask:\n",
    "            state_embed = self.conv(state)\n",
    "            img_mask = self.process(state_embed)\n",
    "            img_mask = img_mask.reshape(-1,1,36,72)\n",
    "            masked_state = state * img_mask # apply mask to full state\n",
    "            pred = self.cnntrunk(masked_state)\n",
    "            return [pred, masked_state, img_mask]\n",
    "        else:\n",
    "            pred = self.cnntrunk(state)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07cbd67-71e4-4c60-8662-070077913cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsize_obs(obs):\n",
    "    cv2.imwrite('temp.jpg', obs)\n",
    "    img = cv2.imread('temp.jpg')\n",
    "    downsized_obs = cv2.resize(img, dsize=(72, 36), interpolation=cv2.INTER_CUBIC)\n",
    "    return downsized_obs\n",
    "\n",
    "def flatten_act(action):\n",
    "    return np.concatenate(list(action.values())).ravel()\n",
    "\n",
    "# reconstructs actions for simulator\n",
    "def reconstruct_act(action):\n",
    "    reconst_action = {}\n",
    "    reconst_action['pose0_position'] = np.array(action[0:2])\n",
    "    reconst_action['pose0_rotation'] = np.array(action[2:6])\n",
    "    reconst_action['pose1_position'] = np.array(action[6:8])\n",
    "    reconst_action['pose1_rotation'] = np.array(action[8:12])\n",
    "    reconst_action = {\n",
    "        k: np.clip(v, env.action_space[k].low, env.action_space[k].high)\n",
    "        for k, v in reconst_action.items()\n",
    "    }\n",
    "    return reconst_action\n",
    "\n",
    "def remove_obj(segm, obs, remove_obj):\n",
    "    #if remove_obj == 'arm':\n",
    "    #    segm = (segm == 2).astype(int)\n",
    "    if remove_obj == 'base':\n",
    "        segm = (segm == 5).astype(int)\n",
    "    elif remove_obj == 'dragged':\n",
    "        segm = (segm == 6).astype(int)\n",
    "    elif remove_obj == 'distractor':\n",
    "        segm = (segm == 7).astype(int)\n",
    "    segm = np.atleast_3d(segm)\n",
    "    \n",
    "    for height in range(128):\n",
    "        for width in range(256):\n",
    "            if segm[height, width] == 1:\n",
    "                obs[height, width] = 47\n",
    "    return obs\n",
    "\n",
    "def compare_actions(actions, gt_action):\n",
    "    comparisons = []\n",
    "    for action in actions:\n",
    "        comparisons.append(np.linalg.norm(gt_action - action[[0,1,6,7]],ord=2))\n",
    "    return comparisons\n",
    "\n",
    "# generates random trajs within specified constraints\n",
    "def gen_trajs(env, num_trajs, task_name, task_kwargs):\n",
    "    trajs = []\n",
    "    task = env.task\n",
    "    oracle_fn = task.oracle(env)\n",
    "    for traj in tqdm(range(num_trajs)):\n",
    "        traj = {'obs': [],'acts': [], 'meta': []}\n",
    "        obs = env.reset()\n",
    "        traj['meta'] = env.meta_info\n",
    "        for step in range(1):\n",
    "            top_obs = obs['rgb']['top'] # extracts top down view only\n",
    "            top_obs = np.rollaxis(top_obs,0,3)\n",
    "            traj['obs'].append(downsize_obs(top_obs.copy()))\n",
    "            # prompt, prompt_assets = env.prompt, env.prompt_assets\n",
    "            oracle_action = oracle_fn.act(obs)\n",
    "            # clip action\n",
    "            oracle_action = {\n",
    "                k: np.clip(v, env.action_space[k].low, env.action_space[k].high)\n",
    "                for k, v in oracle_action.items()\n",
    "            }\n",
    "            traj['acts'].append(flatten_act(oracle_action))\n",
    "            obs, _, done, info = env.step(action=oracle_action, skip_oracle=False)\n",
    "        traj['obs'] = np.array(traj['obs'])\n",
    "        traj['acts'] = np.array(traj['acts'])\n",
    "        traj['meta'] = np.array(traj['meta'])\n",
    "        trajs.append(traj)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccfc2f6-2659-4640-b9d6-0a7546455ef9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "task_name = 'visual_manipulation'\n",
    "#task_kwargs=PARTITION_TO_SPECS[\"train\"][task_name]\n",
    "task_kwargs = { 'num_dragged_obj': 1,\n",
    "     'possible_dragged_obj': ['flower'],\n",
    "     'possible_dragged_obj_texture': ['green'],\n",
    "     'dragged_obj_area': [2],\n",
    "     'num_base_obj': 1,\n",
    "     'possible_base_obj': ['pan'],\n",
    "     'possible_base_obj_texture': ['dark blue'],\n",
    "     'base_obj_area': [3],\n",
    "     'num_distractors_obj': 0,\n",
    "     'possible_distractor_obj_texture': ['yellow'],\n",
    "     'possible_distractor_obj': ['flower'],\n",
    "     'distractor_obj_area' : 3}\n",
    "#record_gui=True, display_debug_window=True, hide_arm_rgb=False\n",
    "env = vima_bench.make(task_name=task_name,task_kwargs=task_kwargs,hide_arm_rgb=False)\n",
    "\n",
    "num_trajs = 10\n",
    "trajs = gen_trajs(env=env, num_trajs=num_trajs, task_name=task_name, task_kwargs=task_kwargs)\n",
    "pickle.dump(trajs, open('trajs.pkl', 'wb'))\n",
    "env.close()\n",
    "#obs = env.reset()\n",
    "#top_obs = obs['rgb']['top']\n",
    "#top_obs = np.rollaxis(top_obs,0,3)\n",
    "#plt.imshow(top_obs)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95620041-1b7d-47f7-a043-eed9b05e824b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fabe86d26a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trajs = pickle.load(open('trajs.pkl','rb'))\n",
    "plt.imshow(trajs[0]['obs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082b88c2-3372-4b12-b64e-3b7d4cf43f4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNPolicy(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (8): Flatten()\n",
       "    (9): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Linear(in_features=160, out_features=100, bias=True)\n",
       "  )\n",
       "  (process): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=1000, out_features=2592, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (cnntrunk): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Flatten()\n",
       "    (10): Linear(in_features=160, out_features=12, bias=True)\n",
       "  )\n",
       "  (mlptrunk): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=100, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tasks = len(trajs)\n",
    "\n",
    "#goal_size = 8 #384\n",
    "act_size = trajs[0]['acts'][0].shape[0]\n",
    "hidden_size = 100\n",
    "mask = False\n",
    "\n",
    "policy = CNNPolicy(act_size, hidden_size, mask=mask)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dde3ee5-ce6a-41eb-ac85-860781776e8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     mask_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.00001\u001b[39m\u001b[38;5;241m*\u001b[39mmasked_state_preds\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     a_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_states\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m action_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(a_preds \u001b[38;5;241m-\u001b[39m t_actions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# supervised learning loss\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask:\n",
      "File \u001b[0;32m~/miniconda3/envs/aligning-construals/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mCNNPolicy.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [pred, masked_state, img_mask]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnntrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/miniconda3/envs/aligning-construals/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/aligning-construals/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/aligning-construals/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 10\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t_idx = np.random.randint(len(trajs), size=(batch_size,)) # Indices of traj\n",
    "        t_idx_pertraj = np.random.randint(1, size=(batch_size,)) # Indices of timesteps in traj\n",
    "        t_states = np.concatenate([trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_actions = np.concatenate([trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "   \n",
    "        t_states = torch.Tensor(t_states).float().to(device)\n",
    "        t_actions = torch.Tensor(t_actions).float().to(device)\n",
    "        \n",
    "        if mask:\n",
    "            a_preds, masked_state_preds, img_mask_preds = policy(t_states)\n",
    "            mask_loss = .00001*masked_state_preds.sum()\n",
    "        else:\n",
    "            a_preds = policy(t_states)[0]\n",
    "        \n",
    "        action_loss = torch.mean(torch.linalg.norm(a_preds - t_actions, dim=-1)) # supervised learning loss\n",
    "        if mask:\n",
    "            loss = action_loss + mask_loss\n",
    "        else:\n",
    "            loss = action_loss\n",
    "            mask_loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(policy.conv[0].weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] mask loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, mask_loss))\n",
    "            print('[%d, %5d] action loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, action_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "torch.save(policy, 'policy.pt')\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bd52a-f3e5-41d7-b327-cb9bf03e20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"visual_manipulation\"\n",
    "task_kwargs = { 'num_dragged_obj': 1,\n",
    "     'possible_dragged_obj': ['pentagon'],\n",
    "     'possible_dragged_obj_texture': ['red'],\n",
    "     'dragged_obj_area': [2],\n",
    "     'num_base_obj': 1,\n",
    "     'possible_base_obj': ['pallet'],#['bowl','square','flower','pentagon','letter E'],\n",
    "     'possible_base_obj_texture': ['blue'],#['yellow','blue','green','tiger','dark green swirl'],\n",
    "     'base_obj_area': [3],\n",
    "     'num_distractors_obj': 0,\n",
    "     'possible_distractor_obj_texture': ['tiger'],\n",
    "     'possible_distractor_obj': ['pan'],\n",
    "     'distractor_obj_area' : [3]}\n",
    "record_cfg = {'save_video': True,\n",
    "     'save_video_path': './rollouts/',\n",
    "     'view': 'front',\n",
    "     'fps': 20,\n",
    "     'video_height': 640,\n",
    "     'video_width': 720}\n",
    "# record_gui=True, display_debug_window=True, hide_arm_rgb=False\n",
    "env = vima_bench.make(task_name=task_name,task_kwargs=task_kwargs,hide_arm_rgb=False,record_cfg=record_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10160fc2-8e9d-4d65-8517-3421c5fae5d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = torch.load('policy.pt')\n",
    "\n",
    "obj_to_remove = None#['base']\n",
    "mask = False\n",
    "policy.eval()\n",
    "num_test_trajs = 1\n",
    "video = True\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(0.3, 0.7)\n",
    "\n",
    "successes = []\n",
    "\n",
    "rollouts = {'actions': [], 'action_starts': [],'action_ends': [], 'true_starts': [], 'true_ends': []}\n",
    "for i in tqdm(range(num_test_trajs)):\n",
    "    os.makedirs('rollouts/' + str(i), exist_ok=True)\n",
    "    obs = env.reset()\n",
    "    rollouts['true_starts'].append(env.task.dragged_pose[0])\n",
    "    rollouts['true_ends'].append(env.task.base_pose[0])\n",
    "    if video:\n",
    "        video_name = str(i)\n",
    "        env.start_rec(video_name)\n",
    "    for step in range(1):\n",
    "        segm = obs['segm']['top']\n",
    "        top_obs = obs['rgb']['top']\n",
    "        top_obs = np.rollaxis(top_obs,0,3)\n",
    "        if obj_to_remove is not None:\n",
    "            obj_removed = random.choice(obj_to_remove)\n",
    "            segm = obs['segm']['top']\n",
    "            top_obs = remove_obj(segm, top_obs, obj_removed)\n",
    "        first_obs = top_obs.copy()\n",
    "        im = Image.fromarray(top_obs)\n",
    "        im.save('rollouts/'+str(i)+\"/\"+str(step)+'.jpg')\n",
    "        top_obs = downsize_obs(top_obs)\n",
    "        state = torch.Tensor(top_obs[None]).to(device)\n",
    "        if mask:\n",
    "            action, masked_state, img_mask = policy(state)\n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            masked_state = masked_state.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            masked_state = Image.fromarray((masked_state * 255).astype(np.uint8))\n",
    "            masked_state.save('rollouts/'+str(i)+\"/\"+str(step)+'-mask.jpg')\n",
    "            img_mask = img_mask.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "        else:\n",
    "            action = policy(state).cpu().detach().numpy()[0]\n",
    "        rollouts['actions'].append(action)\n",
    "        rollouts['action_starts'].append(action[0:2].copy())\n",
    "        rollouts['action_ends'].append(action[6:8].copy())\n",
    "        obs, _, done, info = env.step(action=reconstruct_act(action), skip_oracle=False)\n",
    "        if done:\n",
    "            successes.append(1)\n",
    "        else:\n",
    "            successes.append(0)\n",
    "    if video:\n",
    "        env.end_rec()\n",
    "    top_obs = obs['rgb']['top']\n",
    "    top_obs = np.rollaxis(top_obs,0,3)\n",
    "    if obj_to_remove is not None:\n",
    "        segm = obs['segm']['top']\n",
    "        top_obs = remove_obj(segm, top_obs, obj_removed)\n",
    "    im = Image.fromarray(top_obs)\n",
    "    im.save('rollouts/'+str(i)+\"/\"+str(step+1)+'.jpg')\n",
    "\n",
    "env.close()\n",
    "plt.scatter(np.array(rollouts['true_starts'])[:, 1], np.array(rollouts['true_starts'])[:, 0], marker='o', color='g', label='gt starts')\n",
    "plt.scatter(np.array(rollouts['true_ends'])[:, 1], np.array(rollouts['true_ends'])[:, 0], marker='x', color='g', label='gt ends')\n",
    "plt.scatter(np.array(rollouts['action_starts'])[:, 1], np.array(rollouts['action_starts'])[:, 0], marker='o', color='r', label='rollout starts')\n",
    "plt.scatter(np.array(rollouts['action_ends'])[:, 1], np.array(rollouts['action_ends'])[:, 0], marker='x', color='r', label='rollout ends')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "print(sum(successes)/len(successes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae59c2-c87f-4e2f-a91b-e99144f02302",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d61716-d37b-4296-b93d-39bd26c97ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336d9f0-b371-4a58-ac38-f782648bdab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_action = np.array([ 0.348276  , -0.28591025,  0.5994489 ,  0.13083145])\n",
    "comps = compare_actions(rollouts['actions'], gt_action)\n",
    "print(comps)\n",
    "print(sum(comps)/len(comps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a4693-f77c-43bc-ae56-351eed20ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(obs):\n",
    "    policy.eval()\n",
    "    policy.to(device)\n",
    "    \n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = policy(obs).detach().cpu().numpy()\n",
    "    \n",
    "    dists = np.linalg.norm(logits - gt_action.reshape(1, 12), axis=1).reshape(-1, 1)\n",
    "    # d2 = -dists.copy()\n",
    "    # val = np.concatenate((dists, d2), axis=1)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ee30a-693f-4f5e-b401-f29be7516d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(downsize_obs(first_obs), \n",
    "                                         batch_predict, # prediction function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function\n",
    "print(explanation)\n",
    "print(explanation.intercept)\n",
    "print(explanation.local_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd11541-f5c9-41c2-86d6-805fd168236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation)\n",
    "print(explanation.intercept)\n",
    "print(explanation.local_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5305e-719b-422d-b71b-9f2e9560d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(explanation.segments)\n",
    "print(explanation.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31fd8a-b816-40ff-a932-6c656b7118bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_cam(policy, state):\n",
    "    policy.eval()\n",
    "    policy.to(device)\n",
    "        \n",
    "    state = state/255.0 # process image + switch channels\n",
    "    state = state.permute(0,3,1,2)\n",
    "    conv_maps = policy.cnntrunk[2](policy.cnntrunk[1](policy.cnntrunk[0](state)))\n",
    "    conv_maps = policy.cnntrunk[5](policy.cnntrunk[4](policy.cnntrunk[3](conv_maps)))\n",
    "    conv_maps = policy.cnntrunk[9](policy.cnntrunk[8](policy.cnntrunk[7](policy.cnntrunk[6](conv_maps)))).detach().numpy()[0]\n",
    "    weights = policy.cnntrunk[10].weight.data.cpu().numpy()[0]\n",
    "    \n",
    "    avg_conv_map = np.uint8(conv_maps * weights *255)\n",
    "    avg_conv_map[avg_conv_map < 0] = 0\n",
    "    avg_conv_map = np.array(Image.fromarray(avg_conv_map).resize((256,128)))\n",
    "    \n",
    "    return avg_conv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb35a5-495e-47ea-9de8-d572bcdd9c05",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = grad_cam(policy,state)\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df446a-ee2c-4a01-ad09-41bf30548540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap = cv2.applyColorMap(cv2.resize(output,(256, 128)), cv2.COLORMAP_JET)\n",
    "plt.imshow(heatmap*.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5618114-d080-41a1-a026-c2b2cefa5dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
