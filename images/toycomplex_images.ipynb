{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722f0f6-deef-43b1-bb64-bb1a1f6597b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GCBC with point maze (with multi-goal + color state space)\n",
    "'''\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import io\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode - o\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "lang_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dc18f8-9c07-4770-b61e-2126bbc49078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "                \n",
    "# helper mlp init function\n",
    "def mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod=None):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# MLP policy\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim, action_dim, hidden_dim, hidden_depth, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod)\n",
    "        self.outputs = dict()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "\n",
    "# MLP policy\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, goal_dim, action_dim, hidden_size, mask=False, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.goal = mlp(goal_dim, hidden_size, hidden_dim=0, hidden_depth=0, output_mod=None) # => hidden_size\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,32,kernel_size=8,stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,36)=>(b_size,32,8,8)\n",
    "            nn.Conv2d(32,64,kernel_size=4,stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,8)=>(b_size,64,3,3)\n",
    "            nn.Conv2d(64,32,kernel_size=3,stride=1), nn.LeakyReLU(inplace=True), Flatten(), nn.BatchNorm1d(32), #(b_size,64,3,3)=>(b_size,32,1,1)=>(b_size,32*1*1)\n",
    "            nn.Linear(32*1*1, hidden_size) #(b_size,32*1*1)=>(b_size,hidden_size)\n",
    "        )\n",
    "        self.process = mlp(hidden_size*2, 1*36*36, hidden_dim=1000, hidden_depth=1, output_mod=nn.Sigmoid()) #(b_size,hidden_size*2)=>(b_size,32*1*1)\n",
    "        # UNet deconv mask --------------------------------------------------------------------------------------------------\n",
    "        #self.deconv = nn.Sequential(\n",
    "        #    nn.ConvTranspose2d(32, 64, kernel_size=3, stride=1), nn.LeakyReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32*1*1)=>(b_size,64,3,3)\n",
    "        #    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2), nn.LeakyReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,64,3,3)=>(b_size,32,8,8)\n",
    "        #    nn.ConvTranspose2d(32, 1, kernel_size=8, stride=4), nn.Sigmoid(), #(b_size,32,8,8)=>(b_size,3,36,36)\n",
    "        #)\n",
    "        self.cnntrunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True), nn.BatchNorm2d(32), #(b_size,3,36,36)=>(b_size,32,8,8)\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(64), #(b_size,32,8,8)=>(b_size,64,3,3)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1), nn.LeakyReLU(inplace=True), nn.BatchNorm2d(32), Flatten(), #(b_size,64,3,3)=>(b_size,32,1,1)=>(b_size,32*1*1)\n",
    "            nn.Linear(32*1*1, action_dim)#, nn.LeakyReLU(inplace=True), nn.BatchNorm1d(32), #(b_size,32*1*1)=>(b_size,hidden_size)\n",
    "            #nn.Linear(32, action_dim) #(b_size,hidden_size)=>(b_size,action_dim)\n",
    "        )\n",
    "        self.mlptrunk = mlp(hidden_size*2, action_dim, hidden_dim=100, hidden_depth=1) #(b_size,hidden_size*2)=>(b_size,action_dim)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, goal, state, gt_mask):\n",
    "        state = state/255.0 # process image + switch channels\n",
    "        state = state.permute(0,3,1,2)\n",
    "        state_embed = self.conv(state)\n",
    "        goal_embed = self.goal(goal) # process goal\n",
    "        goal_state = torch.cat((goal_embed,state_embed),dim=1) # process goal + state\n",
    "        \n",
    "        if self.mask:\n",
    "            img_mask = self.process(goal_state)\n",
    "            img_mask = img_mask.reshape(-1,1,36,36)\n",
    "            #img_mask = img_mask.reshape(-1,32,1,1)\n",
    "            #img_mask = self.deconv(img_mask) # deconv for mask\n",
    "            #--------------------- test with gt mask\n",
    "            #img_mask = gt_mask.reshape((-1, 36, 36, 1)) # reshape to 1 channel\n",
    "            #img_mask = np.repeat(img_mask, 3, axis=3) # duplicate across 3 channels\n",
    "            #img_mask = img_mask.permute(0,3,1,2) # process channel switch\n",
    "            #---------------------\n",
    "            masked_state = state * img_mask # apply mask to full state\n",
    "            pred = self.cnntrunk(masked_state)\n",
    "            return [pred, masked_state, img_mask]\n",
    "        else:\n",
    "            pred = self.mlptrunk(goal_state)\n",
    "            return [pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddefccc1-0cd0-4cc4-bf43-e15059ca919f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Goal-conditioned Point Maze (with multi state + color)\n",
    "import gym\n",
    "import gym.wrappers\n",
    "\n",
    "class PointEnvGrid(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.pos = np.array([99., 99.])\n",
    "        self.obj1_pos = np.array([99., 99.])\n",
    "        self.obj1_color = np.array([99., 99., 99., 99.])\n",
    "        self.obj1_shape = np.array([99., 99., 99., 99.])\n",
    "        self.obj2_pos = np.array([99., 99.])\n",
    "        self.obj2_color = np.array([99., 99., 99., 99.])\n",
    "        self.obj2_shape = np.array([99., 99., 99., 99.])\n",
    "        self.goal = np.array(['go to the red X'])\n",
    "        self.goal_pos = np.array([99., 99.])\n",
    "        self.goal_color = np.array([99., 99., 99., 99.])\n",
    "        self.goal_shape = np.array([99., 99., 99., 99.])\n",
    "        self.max_vel = 1.\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            -np.inf * np.ones(14), np.inf * np.ones(14)\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(-np.ones(2), np.ones(2))\n",
    "        self.action_space.seed(0)\n",
    "        self.observation_space.seed(0)\n",
    "\n",
    "    # creates test env with defined goal locations + colors\n",
    "    def reset(self, agent_pos=[0,1,2,3], obj_pos=[0,1,2,3], obj_colors=[0,1,2,3], obj_shapes=[0,1,2,3], scalar=5):\n",
    "        # generates agent+objs without overlap\n",
    "        agent_loc = np.random.choice(agent_pos)\n",
    "        obj_pos = copy.deepcopy(obj_pos)\n",
    "        if agent_loc in obj_pos:\n",
    "            obj_pos.remove(agent_loc)\n",
    "        self.pos = self.gen_pos(agent_loc, scalar)\n",
    "        \n",
    "        obj1_loc, obj2_loc = random.sample(obj_pos, 2)\n",
    "        self.obj1_pos = self.gen_pos(obj1_loc, scalar)\n",
    "        self.obj1_color = self.gen_color(obj_colors)\n",
    "        self.obj1_shape = self.gen_shape(obj_shapes)\n",
    "        self.obj2_pos = self.gen_pos(obj2_loc, scalar)\n",
    "        self.obj2_color = self.gen_color(obj_colors)\n",
    "        self.obj2_shape = self.gen_shape(obj_shapes)\n",
    "\n",
    "        # choose goal from random in objects\n",
    "        goal_obj = random.randint(1, 2)\n",
    "        if goal_obj == 1:\n",
    "            self.goal_pos = self.obj1_pos\n",
    "            self.goal_color = self.obj1_color\n",
    "            self.goal_shape = self.obj1_shape\n",
    "        elif goal_obj == 2:\n",
    "            self.goal_pos = self.obj2_pos\n",
    "            self.goal_color = self.obj2_color\n",
    "            self.goal_shape = self.obj2_shape\n",
    "        self.goal = np.array(['go to the ' + get_color(self.goal_color) + ' ' + get_shape(self.goal_shape)])\n",
    "        return self.get_full_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj1_shape, self.obj2_pos, self.obj2_color, self.obj2_shape, self.goal_color, self.goal_shape]))\n",
    "    \n",
    "    def get_full_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj1_shape, self.obj2_pos, self.obj2_color, self.obj2_shape, self.goal_pos, self.goal_color, self.goal_shape]))\n",
    "    \n",
    "    def step(self, action: np.ndarray):\n",
    "        self.pos += self.max_vel*action\n",
    "        reward = -np.linalg.norm((self.pos - self.goal_pos))\n",
    "        return self.get_full_obs(), reward, False, {}\n",
    "\n",
    "    # given a range of areas, randomly selects and generates position\n",
    "    def gen_pos(self, location, scalar):\n",
    "        if location == 0:\n",
    "            pos = np.array([-1.*scalar, 1.*scalar])\n",
    "        elif location == 1:\n",
    "            pos = np.array([1.*scalar, 1.*scalar])\n",
    "        elif location == 2:\n",
    "            pos = np.array([1.*scalar, -1.*scalar])\n",
    "        elif location == 3:\n",
    "            pos = np.array([-1.*scalar, -1.*scalar])\n",
    "        return pos\n",
    "\n",
    "    def gen_color(self, colors):\n",
    "        one_hot_color = np.zeros(shape=4) # defines total number of colors to choose from\n",
    "        rand_color = random.choice(colors) # samples color from range\n",
    "        one_hot_color[rand_color] = 1.\n",
    "        return one_hot_color\n",
    "\n",
    "    def gen_shape(self, shapes):\n",
    "        one_hot_shape = np.zeros(shape=4)\n",
    "        rand_shape = random.choice(shapes)\n",
    "        one_hot_shape[rand_shape] = 1.\n",
    "        return one_hot_shape\n",
    "\n",
    "class PointEnvComplex(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.pos = np.array([99., 99.])\n",
    "        self.obj1_pos = np.array([99., 99.])\n",
    "        self.obj1_color = np.array([99., 99., 99.])\n",
    "        self.obj2_pos = np.array([99., 99.])\n",
    "        self.obj2_color = np.array([99., 99., 99.])\n",
    "        self.goal = np.array(['go to the red star'])\n",
    "        self.goal_pos = np.array([99., 99.])\n",
    "        self.goal_color = np.array([99., 99., 99.])\n",
    "        self.max_vel = 1.\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            -np.inf * np.ones(14), np.inf * np.ones(14)\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(-np.ones(2), np.ones(2))\n",
    "        self.action_space.seed(0)\n",
    "        self.observation_space.seed(0)\n",
    "\n",
    "    # creates test env with defined goal locations + colors\n",
    "    def reset(self, obj_angles=360, obj_colors=[0,1,2], discretize=10):\n",
    "        self.pos = np.array([0., 0.])\n",
    "        self.obj1_pos, self.obj1_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "        self.obj2_pos, self.obj2_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "\n",
    "        # choose goal from random in objects\n",
    "        goal_obj = random.randint(1, 2)\n",
    "        if goal_obj == 1:\n",
    "            self.goal_pos = self.obj1_pos\n",
    "            self.goal_color = self.obj1_color\n",
    "        elif goal_obj == 2:\n",
    "            self.goal_pos = self.obj2_pos\n",
    "            self.goal_color = self.obj2_color\n",
    "        self.goal = np.array(['go to the ' + get_color(self.goal_color) + ' square'])\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_color]))\n",
    "    \n",
    "    def get_full_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_pos, self.goal_color]))\n",
    "    \n",
    "    def step(self, action: np.ndarray):\n",
    "        self.pos += self.max_vel*action\n",
    "        reward = -np.linalg.norm((self.pos - self.goal_pos))\n",
    "        return self.get_full_obs(), reward, False, {}\n",
    "\n",
    "    # function to generate object based on angles, colors, and discretization ranges\n",
    "    def gen_obj(self, obj_angles, obj_colors, discretize):\n",
    "        angle = np.random.uniform(0, obj_angles) # samples angle from range\n",
    "        angle = round(angle/discretize)*discretize # discretizes to defined range (default 10)\n",
    "        pos = np.array([5*np.cos(np.deg2rad(angle)), 5*np.sin(np.deg2rad(angle))]) # maps to unit circle\n",
    "        \n",
    "        color = np.zeros(shape = 3) # defines total number of colors to choose from\n",
    "        rand_color = random.sample(obj_colors,1) # samples color from range\n",
    "        color[rand_color] = 1.\n",
    "        return pos, color\n",
    "\n",
    "def get_color(color):\n",
    "    if color[0] == 1.:\n",
    "        return 'red'\n",
    "    elif color[1] == 1.:\n",
    "        return 'green'\n",
    "    elif color[2] == 1.:\n",
    "        return 'blue'\n",
    "    elif color[3] == 1.:\n",
    "        return 'yellow'\n",
    "\n",
    "def get_shape(shape):\n",
    "    if shape[0] == 1.:\n",
    "        return 'X'\n",
    "    elif shape[1] == 1.:\n",
    "        return 's'\n",
    "    elif shape[2] == 1.:\n",
    "        return '^'\n",
    "    elif shape[3] == 1.:\n",
    "        return 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84b28c-d26e-4714-ba45-a65852e67dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_env(env):\n",
    "    fig = plt.figure(figsize=(5, 5),facecolor=\"black\")\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(env.pos[0],env.pos[1], marker='o', color='white', s=500) # plots agent\n",
    "    plt.scatter(env.obj1_pos[0], env.obj1_pos[1], marker=get_shape(env.obj1_shape), color=get_color(env.obj1_color), s=500) # plots obj1\n",
    "    plt.scatter(env.obj2_pos[0], env.obj2_pos[1], marker=get_shape(env.obj2_shape), color=get_color(env.obj2_color), s=500) # plots obj2\n",
    "    #plt.scatter(env.goal_pos[0], env.goal_pos[1], marker=get_shape(env.goal_shape), color=get_color(env.goal_color), s=1000) # plots goal\n",
    "    with io.BytesIO() as buff:\n",
    "        fig.savefig(buff, format='raw')\n",
    "        buff.seek(0)\n",
    "        data = np.frombuffer(buff.getvalue(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = data.reshape((int(h), int(w), -1))\n",
    "    img = img[:,:,:3]\n",
    "    plt.close()\n",
    "    return img\n",
    "\n",
    "def plot_full_state(state):\n",
    "    fig = plt.figure(figsize=(.5, .5),facecolor=\"black\")\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(state[0], state[1], marker='o', color='white', s=10) # plots agent\n",
    "    plt.scatter(state[2], state[3], marker=get_shape(state[8:12]), color=get_color(state[4:8]), s=10) # plots obj1\n",
    "    plt.scatter(state[12], state[13], marker=get_shape(state[18:22]), color=get_color(state[14:18]), s=10) # plots obj1\n",
    "    #plt.scatter(state[22], state[23], marker=get_shape(28:32), color=get_color(state[24:28]), s=15) # plots goal\n",
    "    with io.BytesIO() as buff:\n",
    "        fig.savefig(buff, format='raw')\n",
    "        buff.seek(0)\n",
    "        data = np.frombuffer(buff.getvalue(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = data.reshape((int(h), int(w), -1))\n",
    "    img = img[:,:,:3]\n",
    "    plt.close()\n",
    "    return img\n",
    "\n",
    "def plot_img_mask(state):\n",
    "    fig = plt.figure(figsize=(.5, .5),facecolor=\"black\")\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(state[0], state[1], marker='o', color='white', s=10) # plots agent\n",
    "    plt.scatter(state[22], state[23], marker=get_shape(state[28:32]), color=get_color(state[24:28]), s=10) # plots goal\n",
    "    with io.BytesIO() as buff:\n",
    "        fig.savefig(buff, format='raw')\n",
    "        buff.seek(0)\n",
    "        data = np.frombuffer(buff.getvalue(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = data.reshape((int(h), int(w), -1))\n",
    "    img = img[:,:,:3]\n",
    "    img_mask = np.mean(img, axis=2)\n",
    "    img_mask[img_mask > 0] = 1\n",
    "    plt.close()\n",
    "    return img, img_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fce947-8109-4527-ab95-66e5f2202cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointEnvGrid()\n",
    "print(env.reset(agent_pos=[0], obj_pos=[0,1,2,3], obj_colors=[0,1,2,3], obj_shapes=[3], scalar=5))\n",
    "#plt.imshow(plot_env(env))\n",
    "state = plot_full_state(env.get_full_obs())\n",
    "plt.imshow(state)\n",
    "img, img_mask = plot_img_mask(env.get_full_obs())\n",
    "img_mask = img_mask.reshape((36, 36, 1)) #reshape to 1 channel\n",
    "img_mask = np.repeat(img_mask, 3, axis=2)\n",
    "masked_img = img * img_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfdf61-83a4-47dd-9dc2-9a42972d63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e21d63-eea2-404c-ae55-3c8673ecc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates random trajs within specified constraints\n",
    "def gen_trajs(env, num_trajs=1, agent_pos=[0,1,2,3], obj_pos=[0,1,2,3], obj_colors=[0,1,2,3], obj_shapes=[0,1,2,3], scalar=5):\n",
    "    trajs = []\n",
    "    for traj in range(num_trajs):\n",
    "        env.reset(agent_pos=agent_pos, obj_pos=obj_pos, obj_colors=obj_colors, obj_shapes=obj_shapes, scalar=scalar)\n",
    "        #plt.imshow(plot_env(env))\n",
    "        # rolls out a trajectory towards the goal\n",
    "        traj = {'obs': [],'acts': [],'goal': [], 'gt_masks': []}\n",
    "        delta_vector = env.goal_pos - env.pos\n",
    "        obs = plot_full_state(env.get_full_obs())\n",
    "        for i in range(20):\n",
    "            _, img_mask = plot_img_mask(env.get_full_obs())\n",
    "            act = delta_vector * 0.05 # Go in direction between start and end\n",
    "            traj['obs'].append(obs.copy())\n",
    "            traj['acts'].append(act.copy())\n",
    "            one_hot_goal = np.concatenate([env.goal_color, env.goal_shape])\n",
    "            traj['goal'].append(one_hot_goal)\n",
    "            traj['gt_masks'].append(img_mask)\n",
    "            no, r, d, _ = env.step(act)\n",
    "            obs = plot_full_state(no.copy())\n",
    "        traj['obs'] = np.array(traj['obs'])\n",
    "        traj['acts'] = np.array(traj['acts'])\n",
    "        traj['goal'] = np.array(traj['goal'])\n",
    "        traj['gt_masks'] = np.array(traj['gt_masks'])\n",
    "        #plt.plot(traj['obs'][:, 0], traj['obs'][:, 1]) # plots trajs + goals\n",
    "        trajs.append(traj)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab75e2-ae6d-48cd-9d19-f1101a7f5f10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# colors = RGBY\n",
    "# shapes = Xs^d (star, square, triangle, diamond)\n",
    "env = PointEnvGrid()\n",
    "trajs = gen_trajs(env, num_trajs=100, agent_pos=[0,1,2,3], obj_pos=[0,1,2,3], obj_colors=[0,1], obj_shapes=[3], scalar=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a66de7-d994-48b5-bdf2-1bb69556f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "step = 0\n",
    "traj = 0\n",
    "state = trajs[traj]['obs'][step]\n",
    "gt_mask = trajs[traj]['gt_masks'][step]\n",
    "plt.imshow(gt_mask)\n",
    "gt_mask = gt_mask.reshape((36, 36, 1))\n",
    "gt_mask = np.repeat(gt_mask, 3, axis=2) # duplicate across 3 channels\n",
    "final_state = state * gt_mask # apply mask to full state\n",
    "plt.imshow(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2d9d3-2451-4a62-ba47-a0e98581f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(trajs[traj]['obs'][step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9082-083c-446a-92e7-47561103291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = len(trajs)\n",
    "\n",
    "goal_size = 8 #384\n",
    "env = PointEnvGrid()\n",
    "act_size = env.action_space.shape[0]\n",
    "hidden_size = 100\n",
    "mask = True\n",
    "\n",
    "policy = CNNPolicy(goal_size, act_size, hidden_size, mask=mask)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4e3b1-7157-4b74-a7c7-57e878c18936",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 1500\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t_idx = np.random.randint(len(trajs), size=(batch_size,)) # Indices of traj\n",
    "        t_idx_pertraj = np.random.randint(20, size=(batch_size,)) # Indices of timesteps in traj\n",
    "        t_states = np.concatenate([trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_goals = np.concatenate([trajs[c_idx]['goal'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_gt_masks = np.concatenate([trajs[c_idx]['gt_masks'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_actions = np.concatenate([trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "   \n",
    "        t_states = torch.Tensor(t_states).float().to(device)\n",
    "        t_goals = torch.Tensor(t_goals).float().to(device)\n",
    "        #t_goals = torch.Tensor(lang_model.encode(t_goals.ravel())).float().to(device) # embeds goal through language model dim:384\n",
    "        t_gt_masks = torch.Tensor(t_gt_masks).float().to(device)\n",
    "        t_actions = torch.Tensor(t_actions).float().to(device)\n",
    "        \n",
    "        if mask:\n",
    "            a_preds, masked_state_preds, img_mask_preds = policy(t_goals, t_states, t_gt_masks)\n",
    "            mask_loss = 5e-6*masked_state_preds.sum()\n",
    "        else:\n",
    "            a_preds = policy(t_goals, t_states, t_gt_masks)[0]\n",
    "        \n",
    "        action_loss = torch.mean(torch.linalg.norm(a_preds - t_actions, dim=-1)) # supervised learning loss\n",
    "        #construal_loss = 1e-1*((state_mask_preds - t_construals)*(state_mask_preds - t_construals)).sum() # MSE supervised construal loss\n",
    "        if mask:\n",
    "            loss = action_loss + mask_loss\n",
    "        else:\n",
    "            loss = action_loss\n",
    "            mask_loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(policy.conv[0].weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] mask loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, mask_loss))\n",
    "            print('[%d, %5d] action loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, action_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5871ec-78d6-4312-8448-38ce3413b543",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "num_test_trajs = 20\n",
    "# sets sampling for angles and colors\n",
    "agent_pos=[0,1,2,3]\n",
    "obj_pos=[0,1,2,3]\n",
    "obj_colors=[2,3]\n",
    "obj_shapes=[3]\n",
    "scalar=5\n",
    "\n",
    "dist_to_goals = []\n",
    "\n",
    "for i in range(num_test_trajs):\n",
    "    env.reset(agent_pos, obj_pos, obj_colors, obj_shapes, scalar)\n",
    "    one_hot_goal = np.concatenate([env.goal_color, env.goal_shape])\n",
    "    goal = torch.Tensor(one_hot_goal[None]).to(device)\n",
    "    #goal = torch.Tensor(lang_model.encode(env.goal)).to(device)\n",
    "    obs = plot_full_state(env.get_full_obs())\n",
    "    _, gt_mask = plot_img_mask(env.get_full_obs())\n",
    "    \n",
    "    traj = {'obs': [],'acts': [], 'true_goal': [], 'agent_pos': [], 'masked_states': [], 'masks': []}\n",
    "    for i in range(20):\n",
    "        state = torch.Tensor(obs[None]).to(device)\n",
    "        gt_mask = torch.Tensor(gt_mask[None]).to(device)\n",
    "        if mask:\n",
    "            action, masked_state, img_mask = policy(goal,state,gt_mask)\n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            masked_state = masked_state.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            img_mask = img_mask.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            traj['masked_states'].append(masked_state)\n",
    "            traj['masks'].append(img_mask)\n",
    "        else:\n",
    "            action = policy(goal,state,gt_mask)[0].cpu().detach().numpy()[0]\n",
    "        traj['obs'].append(obs.copy())\n",
    "        traj['acts'].append(action.copy())\n",
    "        traj['true_goal'].append(env.goal)\n",
    "        traj['agent_pos'].append(env.pos.tolist())\n",
    "        no, r, d, _ = env.step(action)\n",
    "        obs = plot_full_state(env.get_full_obs())\n",
    "        _, gt_mask = plot_img_mask(env.get_full_obs())\n",
    "    traj['obs'] = np.array(traj['obs'])\n",
    "    traj['acts'] = np.array(traj['acts'])\n",
    "    traj['true_goal'] = np.array(traj['true_goal'])\n",
    "    traj['agent_pos'] = np.array(traj['agent_pos'])\n",
    "    traj['masked_states'] = np.array(traj['masked_states'])\n",
    "    traj['masks'] = np.array(traj['masks'])\n",
    "    dist_to_goals.append(r.copy())\n",
    "    #print(\"Final dist to goal: \", r)\n",
    "    fig = plt.figure(figsize=(5, 5),facecolor=\"black\")\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-6, 6)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    plt.scatter(traj['agent_pos'][0][0], traj['agent_pos'][0][1], marker='o', color='white', s=200) # plots obj1\n",
    "    plt.scatter(env.obj1_pos[0], env.obj1_pos[1], marker=get_shape(env.obj1_shape), color=get_color(env.obj1_color), s=200) # plots obj1\n",
    "    plt.scatter(env.obj2_pos[0], env.obj2_pos[1], marker=get_shape(env.obj2_shape), color=get_color(env.obj2_color), s=200) # plots obj1\n",
    "    #plt.scatter(env.goal_pos[0], env.goal_pos[1], marker='X', color=get_color(env.goal_color), s=1000) # plots goal\n",
    "    plt.plot(traj['agent_pos'][:, 0], traj['agent_pos'][:, 1]) # plots trajs\n",
    "    plt.axis('off')\n",
    "print(env.goal)\n",
    "print(\"Average dist to goal: \", np.mean(dist_to_goals))\n",
    "print(\"Average std: \", np.std(dist_to_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232b53c-9e67-4f14-9348-33b542b21d06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "print(traj['true_goal'][step])\n",
    "state = traj['obs'][step].copy()\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374940c-b2f6-4239-bd9e-7d7fdde30209",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = traj['masks'][step].copy()\n",
    "ax = sns.heatmap(test_mask[:,:,0])\n",
    "plt.show()\n",
    "threshold = 0.8\n",
    "test_mask[test_mask>threshold] = 255\n",
    "test_mask[test_mask<=threshold] = 0\n",
    "masked_state = state * test_mask\n",
    "#plt.imshow(traj['masked_states'][step])\n",
    "plt.imshow(masked_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aa266-2dd0-4b0a-b2d1-24a5b30f8e80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_trajs1 = gen_trajs(env, num_trajs=5, agent_pos=[0,1,2,3], obj_pos=[0,1,2,3], obj_colors=[2,3], scalar=5)\n",
    "finetune_trajs2 = gen_trajs(env, num_trajs=5, agent_pos=[0,1,2,3], obj_pos=[0,1,2,3], obj_colors=[2,3], scalar=5)\n",
    "finetune_trajs = finetune_trajs1 + finetune_trajs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f77bc-d35b-4765-8760-a6610b43aa24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_copy = copy.deepcopy(policy)\n",
    "num_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy_copy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(finetune_trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t_idx = np.random.randint(len(finetune_trajs), size=(batch_size,)) # Indices of traj\n",
    "        t_idx_pertraj = np.random.randint(20, size=(batch_size,)) # Indices of timesteps in traj\n",
    "        t_states = np.concatenate([finetune_trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_goals = np.concatenate([finetune_trajs[c_idx]['goal'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_gt_masks = np.concatenate([finetune_trajs[c_idx]['gt_masks'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "        t_actions = np.concatenate([finetune_trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t_idx, t_idx_pertraj)])\n",
    "   \n",
    "        t_states = torch.Tensor(t_states).float().to(device)\n",
    "        t_goals = torch.Tensor(t_goals).float().to(device)\n",
    "        #t_goals = torch.Tensor(lang_model.encode(t_goals.ravel())).float().to(device) # embeds goal through language model dim:384\n",
    "        t_gt_masks = torch.Tensor(t_gt_masks).float().to(device)\n",
    "        t_actions = torch.Tensor(t_actions).float().to(device)\n",
    "        \n",
    "        if mask:\n",
    "            a_preds, masked_state_preds, img_mask_preds = policy_copy(t_goals, t_states, t_gt_masks)\n",
    "            mask_loss = 5e-6*masked_state_preds.sum()\n",
    "        else:\n",
    "            a_preds = policy_copy(t_goals, t_states, t_gt_masks)[0]\n",
    "        \n",
    "        action_loss = torch.mean(torch.linalg.norm(a_preds - t_actions, dim=-1)) # supervised learning loss\n",
    "        #construal_loss = 1e-1*((state_mask_preds - t_construals)*(state_mask_preds - t_construals)).sum() # MSE supervised construal loss\n",
    "        if mask:\n",
    "            loss = action_loss + mask_loss\n",
    "        else:\n",
    "            loss = action_loss\n",
    "            mask_loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        #print(policy_copy.conv[0].weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] mask loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, mask_loss))\n",
    "            print('[%d, %5d] action loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, action_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156e3c1-6af1-4617-bd8e-913b8374f442",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_copy.eval()\n",
    "num_test_trajs = 20\n",
    "# sets sampling for angles and colors\n",
    "agent_pos=[2,3]\n",
    "obj_pos=[0,1]\n",
    "obj_colors=[0,1,2,3]\n",
    "scalar=5\n",
    "\n",
    "dist_to_goals = []\n",
    "\n",
    "for i in range(num_test_trajs):\n",
    "    env.reset(agent_pos, obj_pos, obj_colors, scalar)\n",
    "    goal = torch.Tensor(env.goal_color[None]).to(device)\n",
    "    #goal = torch.Tensor(lang_model.encode(env.goal)).to(device)\n",
    "    obs = plot_full_state(env.get_full_obs())\n",
    "    _, gt_mask = plot_img_mask(env.get_full_obs())\n",
    "    \n",
    "    traj = {'obs': [],'acts': [], 'true_goal': [], 'agent_pos': [], 'masked_states': [], 'masks': []}\n",
    "    for i in range(20):\n",
    "        state = torch.Tensor(obs[None]).to(device)\n",
    "        gt_mask = torch.Tensor(gt_mask[None]).to(device)\n",
    "        if mask:\n",
    "            action, masked_state, img_mask = policy_copy(goal,state,gt_mask)\n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            masked_state = masked_state.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            img_mask = img_mask.cpu().detach().numpy()[0].transpose((1,2,0))\n",
    "            traj['masked_states'].append(masked_state)\n",
    "            traj['masks'].append(img_mask)\n",
    "        else:\n",
    "            action = policy_copy(goal,state,gt_mask)[0].cpu().detach().numpy()[0]\n",
    "        traj['obs'].append(obs.copy())\n",
    "        traj['acts'].append(action.copy())\n",
    "        traj['true_goal'].append(env.goal)\n",
    "        traj['agent_pos'].append(env.pos.tolist())\n",
    "        no, r, d, _ = env.step(action)\n",
    "        obs = plot_full_state(env.get_full_obs())\n",
    "        _, gt_mask = plot_img_mask(env.get_full_obs())\n",
    "    traj['obs'] = np.array(traj['obs'])\n",
    "    traj['acts'] = np.array(traj['acts'])\n",
    "    traj['true_goal'] = np.array(traj['true_goal'])\n",
    "    traj['agent_pos'] = np.array(traj['agent_pos'])\n",
    "    traj['masked_states'] = np.array(traj['masked_states'])\n",
    "    traj['masks'] = np.array(traj['masks'])\n",
    "    dist_to_goals.append(r.copy())\n",
    "    #print(\"Final dist to goal: \", r)\n",
    "    fig = plt.figure(figsize=(5, 5),facecolor=\"black\")\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-6, 6)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    plt.scatter(traj['agent_pos'][0][0], traj['agent_pos'][0][1], marker='v', color='white', s=200) # plots obj1\n",
    "    plt.scatter(env.obj1_pos[0], env.obj1_pos[1], marker='s', color=get_color(env.obj1_color), s=150) # plots obj1\n",
    "    plt.scatter(env.obj2_pos[0], env.obj2_pos[1], marker='s', color=get_color(env.obj2_color), s=150) # plots obj1\n",
    "    plt.scatter(env.goal_pos[0], env.goal_pos[1], marker='X', color=get_color(env.goal_color), s=1000) # plots goal\n",
    "    plt.plot(traj['agent_pos'][:, 0], traj['agent_pos'][:, 1]) # plots trajs\n",
    "    plt.axis('off')\n",
    "print(\"Average dist to goal: \", np.mean(dist_to_goals))\n",
    "print(\"Average std: \", np.std(dist_to_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e01d6-a654-4f77-b1fb-d198e22fd7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aligning-construals] *",
   "language": "python",
   "name": "conda-env-aligning-construals-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
