{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722f0f6-deef-43b1-bb64-bb1a1f6597b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GCBC with point maze (with multi-goal + color state space)\n",
    "'''\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import io\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode - o\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bed648-e6e2-448d-9f1c-4faa4f1929c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "# helper mlp init function\n",
    "def mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod=None):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f389e-5a73-4c71-ace2-12efbd3057cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP policy\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim, action_dim, hidden_dim, hidden_depth, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod)\n",
    "        self.outputs = dict()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "\n",
    "class MaskedMLPPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim, action_dim, hidden_dim, hidden_depth, default_x, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask = mlp(input_dim, hidden_dim, input_dim, hidden_depth, output_mod=nn.Sigmoid())\n",
    "        self.trunk = mlp(input_dim, output_dim, hidden_dim, hidden_depth, output_mod)\n",
    "        self.default_x = default_x\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.mask(x)\n",
    "        mask = (mask>0.5).float() # convert to binary value\n",
    "        masked_x = (mask*x + (torch.ones(mask.shape)-mask)*torch.Tensor(self.default_x)).float()\n",
    "        return self.trunk(masked_x)\n",
    "    \n",
    "    def mask_loss(self, x):\n",
    "        mask = self.mask(x)\n",
    "        return mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc18f8-9c07-4770-b61e-2126bbc49078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# MLP policy\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self, goal_dim, action_dim, hidden_size, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.goal = mlp(goal_dim, hidden_size, hidden_dim=0, hidden_depth=0, output_mod=None)\n",
    "        self.state = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1), nn.ReLU(inplace=True), Flatten(),\n",
    "            nn.Linear(32, hidden_size), nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.trunk = mlp(hidden_size*2, action_dim, hidden_dim=100, hidden_depth=1)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, goal, x):\n",
    "        goal = self.goal(goal)\n",
    "        state = x.permute(0,3,1,2) # moves channels to pytorch format\n",
    "        state = self.state(state)\n",
    "        full_state = torch.cat((goal,state),dim=1)\n",
    "        return self.trunk(full_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefccc1-0cd0-4cc4-bf43-e15059ca919f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Goal-conditioned Point Maze (with multi state + color)\n",
    "import gym\n",
    "import gym.wrappers\n",
    "\n",
    "class PointEnvComplex(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.pos = np.array([99., 99.])\n",
    "        self.obj1_pos = np.array([99., 99.])\n",
    "        self.obj1_color = np.array([99., 99., 99.])\n",
    "        self.obj2_pos = np.array([99., 99.])\n",
    "        self.obj2_color = np.array([99., 99., 99.])\n",
    "        self.goal_pos = np.array([99., 99.])\n",
    "        self.goal_color = np.array([99., 99., 99.])\n",
    "        self.max_vel = 1.\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            -np.inf * np.ones(14), np.inf * np.ones(14)\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(-np.ones(2), np.ones(2))\n",
    "        self.action_space.seed(0)\n",
    "        self.observation_space.seed(0)\n",
    "\n",
    "    # creates test env with defined goal locations + colors\n",
    "    def reset(self, obj_angles=360, obj_colors=[0,1,2], discretize=10):\n",
    "        self.pos = np.array([0., 0.])\n",
    "        self.obj1_pos, self.obj1_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "        self.obj2_pos, self.obj2_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "\n",
    "        # choose goal from random in objects\n",
    "        goal_obj = random.randint(1, 2)\n",
    "        if goal_obj == 1:\n",
    "            self.goal_pos = self.obj1_pos\n",
    "            self.goal_color = self.obj1_color\n",
    "        elif goal_obj == 2:\n",
    "            self.goal_pos = self.obj2_pos\n",
    "            self.goal_color = self.obj2_color\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_color]))\n",
    "    \n",
    "    def get_full_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_pos, self.goal_color]))\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        self.pos += self.max_vel*action\n",
    "        reward = -np.linalg.norm((self.pos - self.goal_pos))\n",
    "        return self.get_full_obs(), reward, False, {}\n",
    "\n",
    "    # function to generate object based on angles, colors, and discretization ranges\n",
    "    def gen_obj(self, obj_angles, obj_colors, discretize):\n",
    "        angle = np.random.uniform(0, obj_angles) # samples angle from range\n",
    "        angle = round(angle/discretize)*discretize # discretizes to defined range (default 10)\n",
    "        pos = np.array([5*np.cos(np.deg2rad(angle)), 5*np.sin(np.deg2rad(angle))]) # maps to unit circle\n",
    "        \n",
    "        color = np.zeros(shape = 3) # defines total number of colors to choose from\n",
    "        rand_color = random.sample(obj_colors,1) # samples color from range\n",
    "        color[rand_color] = 1.\n",
    "        return pos, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84b28c-d26e-4714-ba45-a65852e67dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(color):\n",
    "    if color[0] == 1.:\n",
    "        return 'red'\n",
    "    elif color[1] == 1.:\n",
    "        return 'blue'\n",
    "    elif color[2] == 1.:\n",
    "        return 'green'\n",
    "        \n",
    "def plot_env(env):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    plt.scatter([env.pos[0]],[env.pos[1]], marker='o', color='black', s=30) # plots agent\n",
    "    plt.scatter([env.obj1_pos[0]],[env.obj1_pos[1]], marker='s', color=get_color(env.obj1_color), s=30) # plots obj1\n",
    "    plt.scatter([env.obj2_pos[0]],[env.obj2_pos[1]], marker='s', color=get_color(env.obj2_color), s=30) # plots obj2\n",
    "    #plt.scatter([env.goal_pos[0]],[env.goal_pos[1]], marker='*', color=get_color(env.goal_color), s=100) # plots goal\n",
    "\n",
    "def plot_full_state(state):\n",
    "    fig = plt.figure(figsize=(.5, .5))\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    plt.scatter(state[0], state[1], marker='o', color='black', s=.25) # plots agent\n",
    "    plt.scatter(state[2], state[3], marker='s', color=get_color(state[4:7]), s=1) # plots obj1\n",
    "    plt.scatter(state[7], state[8], marker='s', color=get_color(state[9:12]), s=1) # plots obj1\n",
    "    #plt.scatter(state[12], state[13], marker='*', color=get_color(state[14:17]), s=5) # plots goal\n",
    "    plt.axis('off')\n",
    "    with io.BytesIO() as buff:\n",
    "        fig.savefig(buff, format='raw')\n",
    "        buff.seek(0)\n",
    "        data = np.frombuffer(buff.getvalue(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = data.reshape((int(h), int(w), -1))\n",
    "    img = img[:,:,:3]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fce947-8109-4527-ab95-66e5f2202cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointEnvComplex()\n",
    "print(env.reset(obj_angles=360, obj_colors=[0,1], discretize=10))\n",
    "plot_env(env)\n",
    "img = plot_full_state(env.get_full_obs())\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e21d63-eea2-404c-ae55-3c8673ecc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates random trajs within specified constraints\n",
    "def gen_trajs(env, num_trajs=1, obj_angles=360, obj_colors=[0,1,2], discretize=10):\n",
    "    trajs = []\n",
    "    for traj in range(num_trajs):\n",
    "        env.reset(obj_angles, obj_colors, discretize)\n",
    "        plot_env(env)\n",
    "        # rolls out a trajectory towards the goal\n",
    "        traj = {'obs': [],'acts': [], 'goal': []}\n",
    "        delta_vector = env.goal_pos\n",
    "        o = plot_full_state(env.get_full_obs())\n",
    "        for i in range(20):\n",
    "            act = delta_vector * 0.05 # Go in direction between start and end\n",
    "            no, r, d, _ = env.step(act)\n",
    "            traj['obs'].append(o.copy())\n",
    "            traj['acts'].append(act.copy())\n",
    "            traj['goal'].append(env.goal_color)\n",
    "            o = plot_full_state(no.copy())\n",
    "        traj['obs'] = np.array(traj['obs'])\n",
    "        traj['acts'] = np.array(traj['acts'])\n",
    "        traj['goal'] = np.array(traj['goal'])\n",
    "        #plt.plot(traj['obs'][:, 0], traj['obs'][:, 1]) # plots trajs + goals\n",
    "        trajs.append(traj)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab75e2-ae6d-48cd-9d19-f1101a7f5f10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = PointEnvComplex()\n",
    "trajs = gen_trajs(env, num_trajs=100, obj_angles=360, obj_colors=[1,2], discretize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a66de7-d994-48b5-bdf2-1bb69556f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = trajs[0]['obs'][0]\n",
    "print(trajs[0]['goal'][0])\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9082-083c-446a-92e7-47561103291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = len(trajs)\n",
    "horizon = 20\n",
    "\n",
    "goal_size = 3\n",
    "env = PointEnvComplex()\n",
    "act_size = env.action_space.shape[0]\n",
    "hidden_size = 100\n",
    "\n",
    "policy = CNNPolicy(goal_size, act_size, hidden_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d98c6-04a5-4e21-8e17-d0caec9b3f7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 20\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t1_idx = np.random.randint(len(trajs), size=(batch_size,)) # Indices of traj\n",
    "        t1_idx_pertraj = np.random.randint(20, size=(batch_size,)) # Indices of timesteps in traj\n",
    "        t1_states = np.concatenate([trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t1_idx, t1_idx_pertraj)])\n",
    "        t1_goals = np.concatenate([trajs[c_idx]['goal'][t_idx][None] for (c_idx, t_idx) in zip(t1_idx, t1_idx_pertraj)])\n",
    "        t1_actions = np.concatenate([trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t1_idx, t1_idx_pertraj)])\n",
    "   \n",
    "        t1_states = torch.Tensor(t1_states).float().to(device)\n",
    "        t1_goals = torch.Tensor(t1_goals).float().to(device)\n",
    "        t1_actions = torch.Tensor(t1_actions).float().to(device)\n",
    "        \n",
    "        a1_pred = policy(t1_goals.to(device), t1_states.to(device)) # action prediction\n",
    "        \n",
    "        #mask_loss = 0.005*policy.mask_loss(t1_states.to(device)) # auxiliary mask loss (maybe need multiplier)\n",
    "        loss = torch.mean(torch.linalg.norm(a1_pred - t1_actions, dim=-1)) # supervised learning loss\n",
    "        #loss = mask_loss + supervised_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            #print('[%d, %5d] mask loss: %.8f' %\n",
    "                  #(epoch + 1, i + 1, mask_loss))\n",
    "            #print('[%d, %5d] action loss: %.8f' %\n",
    "                  #(epoch + 1, i + 1, supervised_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5871ec-78d6-4312-8448-38ce3413b543",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "colors = sns.color_palette(\"hls\", 10)\n",
    "\n",
    "num_test_trajs = 20\n",
    "# sets sampling for angles and colors\n",
    "test_obj_angles = 360\n",
    "test_obj_colors = [0,1,2]\n",
    "discretize = 10\n",
    "\n",
    "dist_to_goals = []\n",
    "\n",
    "for i in range(num_test_trajs):\n",
    "    env.reset(test_obj_angles, test_obj_colors, discretize)\n",
    "    plot_env(env)\n",
    "    goal = torch.Tensor(env.goal_color[None]).to(device)\n",
    "    o = plot_full_state(env.get_full_obs())\n",
    "    #print(\"Goal location: \", env.goal_pos)\n",
    "    #print(\"Goal color: \", get_color(env.goal_color))\n",
    "    \n",
    "    traj = {'obs': [],'acts': [], 'true_goal': []}\n",
    "    for i in range(20):\n",
    "        state = torch.Tensor(o[None]).to(device)\n",
    "        action = policy(goal,state).cpu().detach().numpy()[0]\n",
    "        no, r, d, _ = env.step(action)\n",
    "        traj['obs'].append(o.copy())\n",
    "        traj['acts'].append(action.copy())\n",
    "        traj['true_goal'].append(env.goal_color)\n",
    "        o = plot_full_state(no.copy())\n",
    "    traj['obs'] = np.array(traj['obs'])\n",
    "    traj['acts'] = np.array(traj['acts'])\n",
    "    traj['true_goal'] = np.array(traj['true_goal'])\n",
    "    dist_to_goals.append(r.copy())\n",
    "    print(\"Final dist to goal: \", r)\n",
    "    #plt.plot(traj['obs'][:, 0], traj['obs'][:, 1]) # plots trajs\n",
    "print(\"Average dist to goal: \", np.mean(dist_to_goals))\n",
    "print(\"Average std: \", np.std(dist_to_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac68c2f-841f-4b24-94b9-33c17d70d8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aligning-construals] *",
   "language": "python",
   "name": "conda-env-aligning-construals-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
