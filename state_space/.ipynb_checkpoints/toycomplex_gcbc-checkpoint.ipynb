{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3c17a-aa8e-4169-94d0-f2511072a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GCBC with point maze (with multi-goal + color state space)\n",
    "'''\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode - o\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0523df-da97-4285-8bc3-75e4a0e374e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trunk(x)\n",
    "    \n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk\n",
    "\n",
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca55ae-7328-4dfa-abb2-3adafd1831b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim, hidden_depth, default_x, output_mod=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask = mask(input_dim, hidden_dim, hidden_depth, output_mod=nn.Sigmoid())\n",
    "        self.trunk = mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod)\n",
    "        self.default_x = default_x\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.mask(x)\n",
    "        mask = (mask>0.5).float() # convert to binary value\n",
    "        masked_x = (mask*x + (torch.ones(mask.shape)-mask)*torch.Tensor(self.default_x)).float()\n",
    "        return self.trunk(masked_x)\n",
    "    \n",
    "    def mask_loss(self, x):\n",
    "        mask = self.mask(x)\n",
    "        return mask.sum()\n",
    "\n",
    "def mask(input_dim, hidden_dim, hidden_depth, output_mod=nn.Sigmoid()):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, input_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
    "        mods.append(nn.Linear(hidden_dim, input_dim))\n",
    "    mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9bfd4-d680-492f-899c-535635e52ddd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaskedMLP(15, 1000, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62755b40-529f-4268-947e-dc4c27f06c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal-conditioned Point Maze (with multi state + color)\n",
    "import gym\n",
    "import gym.wrappers\n",
    "\n",
    "class PointEnvComplex(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.pos = np.array([99., 99.])\n",
    "        self.obj1_pos = np.array([99., 99.])\n",
    "        self.obj1_color = np.array([99., 99., 99.])\n",
    "        self.obj2_pos = np.array([99., 99.])\n",
    "        self.obj2_color = np.array([99., 99., 99.])\n",
    "        self.goal_pos = np.array([99., 99.])\n",
    "        self.goal_color = np.array([99., 99., 99.])\n",
    "        self.max_vel = 1.\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            -np.inf * np.ones(14), np.inf * np.ones(14)\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(-np.ones(2), np.ones(2))\n",
    "        self.action_space.seed(0)\n",
    "        self.observation_space.seed(0)\n",
    "\n",
    "    # creates test env with defined goal locations + colors\n",
    "    def reset(self, obj_angles=360, obj_colors=[0,1,2], discretize=10):\n",
    "        self.pos = np.array([0., 0.])\n",
    "        self.obj1_pos, self.obj1_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "        self.obj2_pos, self.obj2_color = self.gen_obj(obj_angles, obj_colors, discretize)\n",
    "\n",
    "        # choose goal from random in objects\n",
    "        goal_obj = random.randint(1, 2)\n",
    "        if goal_obj == 1:\n",
    "            self.goal_pos = self.obj1_pos\n",
    "            self.goal_color = self.obj1_color\n",
    "        elif goal_obj == 2:\n",
    "            self.goal_pos = self.obj2_pos\n",
    "            self.goal_color = self.obj2_color\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_color]))\n",
    "    \n",
    "    def get_full_obs(self):\n",
    "        return copy.deepcopy(np.concatenate([self.pos, self.obj1_pos, self.obj1_color, self.obj2_pos, self.obj2_color, self.goal_pos, self.goal_color]))\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        self.pos += self.max_vel*action\n",
    "        reward = -np.linalg.norm((self.pos - self.goal_pos))\n",
    "        return self.get_obs(), reward, False, {}\n",
    "    \n",
    "    def dist_to_goal(self, state):\n",
    "        return np.linalg.norm(state - self.goal_pos)\n",
    "\n",
    "    # function to generate object based on angles, colors, and discretization ranges\n",
    "    def gen_obj(self, obj_angles, obj_colors, discretize):\n",
    "        angle = np.random.uniform(0, obj_angles) # samples angle from range\n",
    "        angle = round(angle/discretize)*discretize # discretizes to defined range (default 10)\n",
    "        pos = np.array([5*np.cos(np.deg2rad(angle)), 5*np.sin(np.deg2rad(angle))]) # maps to unit circle\n",
    "        \n",
    "        color = np.zeros(shape = 3) # defines total number of colors to choose from\n",
    "        rand_color = random.sample(obj_colors,1) # samples color from range\n",
    "        color[rand_color] = 1.\n",
    "        return pos, color\n",
    "\n",
    "    # function to retrieve plotable color from one-hot encoding\n",
    "    def get_color(self, color):\n",
    "        if color[0] == 1.:\n",
    "            return 'red'\n",
    "        elif color[1] == 1.:\n",
    "            return 'blue'\n",
    "        elif color[2] == 1.:\n",
    "            return 'green'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ce122-d461-44d3-ba4a-79d48c3a5b9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_env(env):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    plt.scatter([env.pos[0]],[env.pos[1]], marker='o', color='black', s=30) # plots agent\n",
    "    plt.scatter([env.obj1_pos[0]],[env.obj1_pos[1]], marker='s', color=env.get_color(env.obj1_color), s=30) # plots obj1\n",
    "    plt.scatter([env.obj2_pos[0]],[env.obj2_pos[1]], marker='s', color=env.get_color(env.obj2_color), s=30) # plots obj2\n",
    "    plt.scatter([env.goal_pos[0]],[env.goal_pos[1]], marker='*', color=env.get_color(env.goal_color), s=100) # plots goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc94257-43bf-4585-8fac-ab62bdf8e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointEnvComplex()\n",
    "print(env.reset(obj_angles=360, obj_colors=[0,1,2], discretize=10))\n",
    "plot_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccd498-2d9c-4c31-95e3-a538395018a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates random trajs within specified constraints\n",
    "def gen_trajs(env, num_trajs=1, obj_angles=360, obj_colors=[0,1,2], discretize=10):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    trajs = []\n",
    "    for traj in range(num_trajs):\n",
    "        env.reset(obj_angles, obj_colors, discretize)\n",
    "        plot_env(env)\n",
    "        # rolls out a trajectory towards the goal\n",
    "        traj = {'obs': [],'acts': [], 'next_obs': []}\n",
    "        delta_vector = env.goal_pos\n",
    "        o = env.get_obs()\n",
    "        for i in range(100):\n",
    "            act = delta_vector * 0.01 # Go in direction between start and end\n",
    "            no, r, d, _ = env.step(act)\n",
    "            traj['obs'].append(o.copy())\n",
    "            traj['acts'].append(act.copy())\n",
    "            traj['next_obs'].append(no.copy())\n",
    "            o = no\n",
    "        traj['obs'] = np.array(traj['obs'])\n",
    "        traj['acts'] = np.array(traj['acts'])\n",
    "        traj['next_obs'] = np.array(traj['next_obs'])\n",
    "        \n",
    "        plt.plot(traj['obs'][:, 0], traj['obs'][:, 1]) # plots trajs + goals\n",
    "        \n",
    "        trajs.append(traj)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ebd53-8300-4db2-9cb0-965d909bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointEnvComplex()\n",
    "trajs = gen_trajs(env, num_trajs=1000, obj_angles=90, obj_colors=[0,1,2], discretize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdc52a-3862-45d6-9ebb-460fdd3a0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward model\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim, hidden_depth):\n",
    "        super().__init__()\n",
    "        self.trunk = mlp(obs_dim, hidden_dim, action_dim, hidden_depth)\n",
    "        self.outputs = dict()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        next_pred = self.trunk(obs)\n",
    "        return next_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b95f1-d3de-4966-94aa-cf126a66f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 1000\n",
    "hidden_depth = 3\n",
    "horizon = 100\n",
    "env = PointEnvComplex()\n",
    "obs_size = env.get_obs().shape[0]\n",
    "act_size = env.action_space.shape[0]\n",
    "default_x = env.get_obs()\n",
    "#policy = MaskedMLP(obs_size, hidden_layer_size, act_size, hidden_depth, default_x)\n",
    "policy = Policy(obs_size, act_size, hidden_layer_size, hidden_depth)\n",
    "num_tasks = len(trajs)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77975ce7-8dfd-4b4d-84da-7f9c0033c55f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(policy.parameters()))\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(trajs)))\n",
    "\n",
    "num_batches = len(idxs) // batch_size\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    np.random.shuffle(idxs)\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t1_idx = np.random.randint(len(trajs), size=(batch_size,)) # Indices of traj\n",
    "        t1_idx_pertraj = np.random.randint(100, size=(batch_size,))\n",
    "        t1_states = np.concatenate([trajs[c_idx]['obs'][t_idx][None] for (c_idx, t_idx) in zip(t1_idx, t1_idx_pertraj)])\n",
    "        t1_actions = np.concatenate([trajs[c_idx]['acts'][t_idx][None] for (c_idx, t_idx) in zip(t1_idx, t1_idx_pertraj)])\n",
    "   \n",
    "        t1_states = torch.Tensor(t1_states).float().to(device)\n",
    "        t1_actions = torch.Tensor(t1_actions).float().to(device)\n",
    "        \n",
    "        a1_pred = policy(t1_states.to(device)) # action prediction\n",
    "        \n",
    "        #mask_loss = 0.005*policy.mask_loss(t1_states.to(device)) # auxiliary mask loss (maybe need multiplier)\n",
    "        loss = torch.mean(torch.linalg.norm(a1_pred - t1_actions, dim=-1)) # supervised learning loss\n",
    "        #loss = mask_loss + supervised_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            #print('[%d, %5d] mask loss: %.8f' %\n",
    "                  #(epoch + 1, i + 1, mask_loss))\n",
    "            #print('[%d, %5d] action loss: %.8f' %\n",
    "                  #(epoch + 1, i + 1, supervised_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fb137-bad5-4c2b-b4b6-aef45c0d36e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve plotable color from one-hot encoding\n",
    "def get_color(color):\n",
    "    if color[0] == 1.:\n",
    "        return 'red'\n",
    "    elif color[1] == 1.:\n",
    "        return 'blue'\n",
    "    elif color[2] == 1.:\n",
    "        return 'green'\n",
    "\n",
    "def plot_state(state):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    plt.scatter(state[0], state[1], marker='o', color='black', s=30) # plots agent\n",
    "    plt.scatter(state[2], state[3], marker='s', color=get_color(state[4:7]), s=30) # plots obj1\n",
    "    plt.scatter(state[7], state[8], marker='s', color=get_color(state[9:12]), s=30) # plots obj2\n",
    "    plt.scatter(-9.5, -9.5, marker='*', color=get_color(state[12:]), s=100) # plots goal with only color\n",
    "\n",
    "# returns masked out state_hat\n",
    "def gen_construal(policy, state, default_x):\n",
    "    mask = policy.mask(state)\n",
    "    mask = (mask>0.5).float() # convert to binary values\n",
    "    print(\"Mask \", mask)\n",
    "    masked_x = (mask*state + (1-mask)*default_x).float()\n",
    "    print (\"Abstract state \", masked_x)\n",
    "    return masked_x.cpu().detach().flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df209b71-285e-4a7d-ba09-b391aea49a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "colors = sns.color_palette(\"hls\", 10)\n",
    "\n",
    "num_test_trajs = 1\n",
    "dist_to_goals = []\n",
    "# sets sampling for angles and colors\n",
    "test_obj_angles = 360\n",
    "test_obj_colors = [0,1,2]\n",
    "discretize = 10\n",
    "\n",
    "for i in range(num_test_trajs):\n",
    "    env.reset(test_obj_angles, test_obj_colors, discretize)\n",
    "    print(\"Goal location: \", env.goal_pos)\n",
    "    print(\"Goal color: \", env.get_color(env.goal_color))\n",
    "    plot_env(env)\n",
    "    o = env.get_obs()\n",
    "    traj = {'obs': [],'acts': [], 'next_obs': []}\n",
    "    for i in range(100):\n",
    "        if i == 0:\n",
    "            first_state = torch.Tensor(o[None]).to(device)\n",
    "        state = torch.Tensor(o[None]).to(device)\n",
    "        action = policy(state).cpu().detach().numpy()[0]\n",
    "        no, r, d, _ = env.step(action)\n",
    "        traj['obs'].append(o.copy())\n",
    "        traj['acts'].append(action.copy())\n",
    "        traj['next_obs'].append(no.copy())\n",
    "        o = no.copy()\n",
    "    traj['obs'] = np.array(traj['obs'])\n",
    "    traj['acts'] = np.array(traj['acts'])\n",
    "    traj['next_obs'] = np.array(traj['next_obs'])\n",
    "    dist_to_goal = np.linalg.norm(traj['obs'][-1][0:2] - env.goal_pos) # extracts rollout pos and calculates dist to goal\n",
    "    dist_to_goals.append(dist_to_goal)\n",
    "    print(\"Dist to goal: \", dist_to_goal)\n",
    "    plt.plot(traj['obs'][:, 0], traj['obs'][:, 1]) # plots trajs\n",
    "print(\"Average dist to goal: \", np.mean(dist_to_goals))\n",
    "print(\"Average std: \", np.std(dist_to_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07727147-e025-4783-a12e-36b4cf3a6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_state)\n",
    "plot_state(gen_construal(policy, first_state, default_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1b88d-d3d5-4140-8441-82dcf7c8e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(state):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    colors = sns.color_palette(\"hls\", 10)\n",
    "    \n",
    "    plt.scatter(state[0], state[1], marker='o', color='black', s=30) # plots agent\n",
    "    plt.scatter(state[2], state[3], marker='*', color=get_color(state[4:7]), s=100) # plots obj1\n",
    "    #plt.scatter(state[7], state[8], marker='s', color=get_color(state[9:12]), s=30) # plots obj2\n",
    "    \n",
    "state = np.array([0.0000,  0.0000,  4.3301, -2.5000,  0.0000,  1.0000,  0.0000, 0.000,\n",
    "          0.000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000])\n",
    "plot_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9bb0b-9317-4e42-a129-7af35ecb8fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aligning-construals] *",
   "language": "python",
   "name": "conda-env-aligning-construals-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
